# Как работает проект - Полный Workflow

## Общая схема работы

Проект работает в два этапа:

1. **Подготовка** - клонирование репозитория и создание векторной базы для проекта
2. **Использование** - разрешение ошибок из stack trace с указанием проекта

### Ключевые особенности

- **Изоляция проектов**: Каждый проект имеет свою собственную векторную базу в `./vector_store/{project_name}/`
- **Группировка векторов**: Документы группируются по проектам, что предотвращает смешивание кода разных проектов
- **Точное разрешение**: При разрешении ошибки используются только файлы из указанного проекта

---

## Этап 1: Подготовка (POST /clone)

### Шаг 1: Клонирование репозитория

```bash
POST /clone
{
  "url": "git@github.com:user/repo.git",
  "branch": "main",
  "project_name": "my-project"
}
```

**Что происходит:**
- Репозиторий клонируется по URL (SSH или HTTPS)
- Репозиторий клонируется в `./cloned_repos/{repo_name}/`
- Векторная база создаётся для проекта `my-project` в `./vector_store/my-project/`
- Если векторная база проекта уже существует, новые документы добавляются к ней

### Шаг 2: Индексация Python-файлов

**Модуль:** `agent/indexer.py` → `extract_python_files()`

**Процесс:**
1. Рекурсивно обходит все файлы в репозитории
2. Ищет только `*.py` файлы
3. Игнорирует директории: `.git`, `node_modules`, `venv`, `.idea`, `build`, `__pycache__`, и т.д.
4. **Разбиение на чанки (chunking):**
   - Большие файлы (>500 строк) разбиваются на чанки по 500 строк
   - Перекрытие между чанками: 50 строк (для сохранения контекста)
   - Маленькие файлы (≤500 строк) остаются целиком
5. Для каждого чанка создаётся отдельный `Document`:
   ```python
   Document(
       page_content="<содержимое чанка>",
       metadata={
           "path": "filename.py",
           "file_path": "path/to/filename.py",
           "chunk_index": 0,
           "start_line": 1,
           "end_line": 500,
           "total_chunks": 3
       }
   )
   ```

**Преимущества chunking:**
- **Точность**: Находим конкретные участки кода, а не целые файлы
- **Эффективность**: Для больших файлов индексируются только релевантные части
- **Контекст**: Перекрытие сохраняет связь между соседними участками кода

**Результат:** Список документов LangChain (каждый чанк = отдельный документ)

### Шаг 3: Создание эмбеддингов

**Модуль:** `agent/vecstore.py` → `create_vector_store()`

**Процесс:**
1. Для каждого документа создаётся эмбеддинг через **OpenRouter API**:
   - Модель: `text-embedding-ada-002` (настраивается через `OPENROUTER_EMBEDDING_MODEL`)
   - API: `https://openrouter.ai/api/v1`
   - Каждый файл преобразуется в вектор (обычно 1536 измерений)

2. Все эмбеддинги сохраняются в **FAISS** векторную базу:
   - FAISS создаёт индекс для быстрого поиска похожих векторов
   - Индекс сохраняется в `./vector_store/{project_name}/`

**Результат:** Векторная база данных FAISS с эмбеддингами всех Python-файлов проекта

---

## Этап 2: Разрешение ошибки (POST /resolve)

### Шаг 1: Парсинг stack trace

**Модуль:** `agent/resolver.py` → `parse_stack_trace()`

```python
# Пример stack trace:
"""
Traceback (most recent call last):
  File "/path/to/file.py", line 42, in function_name
    result = some_function()
  File "/path/to/other_file.py", line 10, in some_function
    return value / 0
ZeroDivisionError: division by zero
"""
```

**Что извлекается:**
- Имена файлов: `file.py`, `other_file.py`
- Номера строк: `42`, `10`

**Результат:**
```python
[
    {"file": "file.py", "line": 42},
    {"file": "other_file.py", "line": 10}
]
```

### Шаг 2: Поиск релевантных документов

**Модуль:** `agent/resolver.py` → `get_relevant_docs()`

**Процесс (гибридный подход):**

1. **Точное сопоставление по пути:**
   - Сначала ищет файлы по точному пути из stack trace
   - Сравнивает полный путь, относительный путь и имя файла
   - Точные совпадения получают максимальный приоритет (score = 1.0)

2. **Semantic search (если точное совпадение не найдено):**
   - Используется `vector_store.similarity_search_with_score(file_name, k=3)`
   - FAISS ищет наиболее похожие документы по векторному сходству
   - Результаты ранжируются по релевантности (score)

3. **Оптимизация:**
   - Один объединённый запрос для всех файлов из stack trace
   - Затем фильтрация по точным путям
   - Semantic search только для файлов без точного совпадения

4. Все найденные документы объединяются и сортируются по релевантности (без дублей)

**Результат:** Отсортированный список релевантных документов (чанков) из кодовой базы

### Шаг 3: Построение контекста

**Модуль:** `agent/resolver.py` → `build_context()`

**Процесс:**

1. **Для файлов из stack trace с номерами строк:**
   - Если найден чанк, содержащий проблемную строку → показывается весь чанк
   - Проблемные строки выделяются маркером `>>>`
   - Номера строк отображаются слева для удобства
   - Пример:
     ```
     === Файл: utils.py [чанк 1/3, строки 1-500] (строки из stack trace: 42) ===
        40 | def process_data():
        41 |     data = load()
     >>> 42 |     result = data.process()  # <-- проблемная строка
        43 |     return result
     ```

2. **Для файлов, найденных через semantic search:**
   - Если это чанк → показывается весь чанк целиком
   - Если это целый файл → показываются первые 100 строк как превью

3. **Ограничения:**
   - Максимум 150,000 токенов (~600,000 символов)
   - При превышении лимита контекст обрезается

4. **Формат вывода:**
   ```
   === Файл: file.py [чанк 1/2, строки 1-500] (строки из stack trace: 42, 58) ===
   >>>   42 |     result = some_function()
         43 |     return result
         ...
   >>>   58 |     raise HTTPException(...)
   
   === Файл: other_file.py [чанк 1/1, строки 1-150] (найден через similarity search) ===
   <содержимое чанка>
   ```

**Результат:** Оптимизированный контекст с релевантными участками кода и выделенными проблемными строками

### Шаг 4: Генерация ответа через LLM

**Модуль:** `agent/resolver.py` → `resolve_error()`

**Процесс:**
1. Формируется промпт:
   ```
   Вот ошибка из stack trace:
   <stack trace>
   
   Вот релевантные файлы из кодовой базы:
   <контекст>
   
   Проанализируй ошибку и:
   1. Объясни причину ошибки
   2. Предложи конкретное исправление
   3. Предоставь исправленный код
   ```

2. Отправляется запрос в **OpenRouter API**:
   - Модель: `openai/gpt-3.5-turbo` (настраивается через `OPENROUTER_LLM_MODEL`)
   - API: `https://openrouter.ai/api/v1`
   - LLM анализирует ошибку и контекст кода

3. LLM возвращает объяснение и предложение исправления

**Результат:** Текстовый ответ с объяснением ошибки и предложением исправления

---

## Визуальная схема

```
┌─────────────────────────────────────────────────────────┐
│                    POST /clone                           │
└─────────────────────────────────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  1. Клонирование репозитория │
        │     git clone / git pull      │
        └───────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  2. Индексация Python-файлов  │
        │     extract_python_files()     │
        │     → Chunking (500 строк)     │
        │     → List[Document] (чанки)   │
        └───────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  3. Создание эмбеддингов      │
        │     OpenRouter API            │
        │     → Векторы (1536 dim)      │
        └───────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  4. Сохранение в FAISS       │
        │     create_vector_store()     │
        │     → ./vector_store/{project}/│
        └───────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│                    POST /resolve                         │
└─────────────────────────────────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  1. Парсинг stack trace       │
        │     parse_stack_trace()       │
        │     → [{"file": "...",        │
        │         "line": 42}]          │
        └───────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  2. Поиск релевантных файлов  │
        │     Гибридный поиск:          │
        │     - Точное сопоставление    │
        │     - Semantic search          │
        │     FAISS векторный поиск     │
        └───────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  3. Построение контекста      │
        │     build_context()           │
        │     - Выделение проблемных     │
        │       строк из чанков          │
        │     → Оптимизированный текст  │
        └───────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  4. Генерация ответа          │
        │     OpenRouter LLM API         │
        │     → Объяснение + исправление│
        └───────────────────────────────┘
```

---

## Пример полного цикла

### 1. Подготовка

```bash
curl -X POST "http://localhost:8000/clone" \
  -H "Content-Type: application/json" \
  -d '{
    "ssh_url": "git@github.com:myuser/myrepo.git",
    "branch": "main",
    "project_name": "my-project"
  }'
```

**Что происходит внутри:**
1. Клонируется `myrepo` → `./cloned_repos/myrepo/`
2. Найдено 50 Python-файлов
3. Файлы разбиты на чанки (например, 50 файлов → 120 чанков)
4. Создано 120 эмбеддингов через OpenRouter API (по одному на чанк)
5. Сохранено в FAISS → `./vector_store/my-project/`

### 2. Использование

```bash
curl -X POST "http://localhost:8000/resolve" \
  -H "Content-Type: application/json" \
  -d '{
    "stacktrace": "Traceback... File \"utils.py\", line 15...",
    "project_name": "my-project"
  }'
```

**Что происходит внутри:**
1. Загружена векторная база проекта `my-project`
2. Извлечено: `{"file": "utils.py", "line": 15, "full_path": "/path/to/utils.py"}`
3. Найдены релевантные чанки:
   - Точное совпадение: чанк из `utils.py`, содержащий строку 15
   - Semantic search: дополнительные релевантные чанки
4. Собран контекст с выделением проблемной строки 15
5. LLM проанализировал конкретный участок кода и вернул ответ

---

## Ключевые технологии

- **FAISS** - быстрый векторный поиск (Facebook AI Similarity Search)
- **OpenRouter API** - для эмбеддингов и LLM
- **LangChain** - работа с документами и векторными базами
- **FastAPI** - REST API сервер

## Преимущества подхода

1. **Изоляция проектов** - каждый проект имеет свою векторную базу, предотвращая смешивание кода
2. **Chunking** - большие файлы разбиваются на чанки, что позволяет находить точные участки кода
3. **Гибридный поиск** - комбинация точного сопоставления и semantic search для максимальной точности
4. **Умное построение контекста** - выделение проблемных строк и показ только релевантных участков кода
5. **Векторный поиск** - находит релевантные чанки даже если имя файла не совпадает точно
6. **Масштабируемость** - FAISS эффективно работает с большими кодовыми базами
7. **Автоматизация** - весь процесс от клонирования до решения ошибки

